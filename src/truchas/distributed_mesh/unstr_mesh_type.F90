!!
!! UNSTR_MESH_TYPE
!!
!! This module provides a derived type that encapsulates the data describing a
!! distributed unstructured mixed-element mesh.  Supported element types are
!! hexes, tets, pyramids, and wedges/prisms.
!!
!! Neil N. Carlson <nnc@lanl.gov>
!! Revised May 2015
!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!
!! Copyright (c) Los Alamos National Security, LLC.  This file is part of the
!! Truchas code (LA-CC-15-097) and is subject to the revised BSD license terms
!! in the LICENSE file found in the top-level directory of this distribution.
!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!
!! PROGRAMMING INTERFACE
!!
!!  The module defines the derived type UNSTR_MESH that encapsulates the data
!!  describing a distributed unstructured mixed-element mesh.  On each process
!!  the object describes a complete mesh of some subdomain that references only
!!  local entities (cells, faces, nodes), and can rightly be considered as a
!!  serial mesh for that subdomain.  Globally, the mesh-conforming subdomains
!!  will overlap, perhaps only along a boundary but more generally on some
!!  collection of cells.  Additional mesh data describes this overlap and
!!  provides for communication between overlapping entities.
!!
!!  Objects of this type are intended to be used by trusted code, and so its
!!  data components are public.  However, the components must be treated as
!!  read-only because an object may be shared amongst multiple clients.  The
!!  following data components are accessible:
!!
!!    nnode, nface, ncell - the number of nodes, faces, and cells in the mesh.
!!
!!    xcnode, cnode - pair of rank-1 integer arrays storing the cell-node data:
!!        cnode(xcnode(j):xcnode(j+1)-1) is the ordered list of node indices
!!        defining cell j.  The shape of xcnode is [ncell+1] and the shape of
!!        cnode is [xcnode(ncell+1)-1].
!!
!!    xcface, cface - pair of rank-1 integer arrays storing the cell-face data:
!!        cface(xcface(j):xcface(j+1)-1) is the ordered list of face indices
!!        belonging to cell j.  The shape of xcface is [ncell+1] and the shape
!!        of cface is [xcface(ncell+1)-1].
!!
!!    xfnode, fnode - pair of rank-1 integer arrays storing the face-node data:
!!        fnode(xfnode(j):xfnode(j+1)-1) is the ordered list of node indices
!!        defining the oriented face j.  The shape of xfnode is [nface+1] and
!!        the shape of fnode is [xfnode(nface+1)-1].
!!
!!    cfpar - an integer bit mask array storing the relative cell face
!!        orientations: btest(cfpar(j),k) is true when face k of cell j is
!!        inward oriented with respect to cell j, and false when it is
!!        outward oriented.  The shape of cfpar is [ncell].
!!
!!    xnode - a rank-1 integer array giving the mapping from local node indices
!!        to their external (global) index (as defined in the mesh file, for
!!        example).  Its shape is [nnode].
!!
!!    xcell - a rank-1 integer array giving the mapping from local cell indices
!!        to their external (global) index (as defined in the mesh file, for
!!        example).  Its shape is [ncell].
!!
!!  PARALLEL DATA:
!!
!!    nnode_onP, nface_onP, ncell_onP - the number of local nodes, faces, and
!!        cells that that are uniquely owned (on-process).
!!
!!    node_ip, face_ip, cell_ip - derived types that describe the partitioning
!!        and overlap of nodes, edges, faces, and cells, including information
!!        necessary to communicate off-process data between processes.
!!
!!  MESH ENTITY DATA:
!!
!!    cell_set_id - a rank-1 integer array storing the unique cell set IDs.
!!        This data is replicated on each process.
!!
!!    cell_set_mask - a rank-1 bitmask array: btest(cell_set_mask(j),k)
!!        returns true if cell j belongs to the cell set with ID cell_set_id(k).
!!
!!    face_set_id - a rank-1 integer array storing the unique face set IDs.
!!        This data is replicated on each process.
!!
!!    face_set_mask - a rank-1 bitmask array: btest(face_set_mask(j),k) returns
!!        true if face j belongs to the face set with ID face_set_id(k).
!!        Btest(face_set_mask(j),0) returns true if face j is a boundary face (global mesh).
!!
!!    node_set_id - a rank-1 integer array storing the unique node set IDs.
!!        This data is replicated on each process.
!!
!!    node_set_mask - a rank-1 bitmask array: btest(node_set_mask(j),k) returns
!!        true if node j belongs to the node set with ID node_set_id(k).
!!
!!  GEOMETRY DATA:
!!
!!    x - the rank-2 real array of node coordinates; x(:,j) is the position in
!!        R^3 of node j.  Its shape is [3,nnode].
!!
!!    area - the rank-1 real array of face areas; area(j) is the area of face j.
!!        Its shape is [nface].
!!
!!    volume - the rank-1 real array of signed cell volumes; volume(j) is the
!!        signed volume of cell j.  Its shape is [ncell].
!!
!!    normal - the rank-2 real array of oriented face areas; normal(:,j) is the
!!        oriented area of face j.  Its shape is [3,nface].
!!
!!  INTERFACE LINKS:
!!
!!    nlink, nlink_onP - the number of interface links and uniquely owned
!!        (on-process) interface links in the (subdomain) mesh.
!!
!!    lface - the rank-1 integer link-face array: lface(:,j) are the indices
!!        of the two opposing mesh faces for link j.
!!
!!    link_set_id - a rank-1 integer array storing the unique link set IDs.
!!        This data is replicated on each process.
!!
!!    link_set_mask - a rank-1 bitmask array: btest(link_set_mask(j),k)
!!        returns true if link j belongs to the link set with ID link_set_id(k).
!!
!!    link_ip - derived type that describes the partitioning and overlap of
!!        links, including information necessary to comminicate off-process
!!        data between processes.
!!

#include "f90_assert.fpp"

module unstr_mesh_type

  use kinds, only: r8
  use base_mesh_class
  use index_partitioning
  use parallel_communication
  use bitfield_type
  implicit none
  private

  type, extends(base_mesh), public :: unstr_mesh
    integer, allocatable :: xcnode(:), cnode(:) ! cell nodes
    integer, allocatable :: xcface(:), cface(:) ! cell faces
    integer, allocatable :: xfnode(:), fnode(:) ! face nodes
    integer, allocatable :: xcnhbr(:), cnhbr(:) ! cell neighbors
    integer, allocatable :: cfpar(:)  ! relative cell face orientation (bit mask)
    real(r8), allocatable :: normal(:,:)
    !! Mesh interface links.
    integer :: nlink = 0, nlink_onP = 0
    integer, allocatable :: lface(:,:)        ! pointer due to localize_index_array
    integer, allocatable :: link_set_id(:)    ! user-assigned ID for each link block
    type(bitfield), allocatable :: link_set_mask(:)  ! link block index
    type(ip_desc) :: link_ip
    !! Additional link data aiding transition from old mesh.
    integer, allocatable :: link_cell_id(:)   ! external cell ID the link was derived from (or 0)
    integer, allocatable :: lnhbr(:,:)        ! link cell neighbors (2)
    integer, allocatable :: xlnode(:), lnode(:) ! link nodes
    integer, allocatable :: parent_node(:)    ! node parents (global ID)
  contains
    procedure :: get_global_cnode_array
    procedure :: get_global_cface_array
    procedure :: compute_geometry
    procedure :: write_profile
  end type unstr_mesh

contains

  !! Compute the geometric data components from the node coordinates.
  subroutine compute_geometry (this)
    use cell_geometry, only: cell_volume, face_normal, vector_length
    class(unstr_mesh), intent(inout) :: this
    integer :: j
    ASSERT(allocated(this%volume))
    ASSERT(allocated(this%normal))
    ASSERT(allocated(this%area))
    do j = 1, this%ncell
      associate (cell_nodes => this%cnode(this%xcnode(j):this%xcnode(j+1)-1))
        this%volume(j) = cell_volume(this%x(:,cell_nodes))
      end associate
    end do
    do j = 1, this%nface
      associate(face_nodes => this%fnode(this%xfnode(j):this%xfnode(j+1)-1))
        this%normal(:,j) = face_normal(this%x(:,face_nodes))
        this%area(j) = vector_length(this%normal(:,j))
      end associate
    end do
  end subroutine compute_geometry

  !! Creates the global ragged CNODE array on the IO process, 0-sized on others.
  subroutine get_global_cnode_array (this, xcnode, cnode)
    class(unstr_mesh), intent(in) :: this
    integer, allocatable, intent(out) :: xcnode(:), cnode(:)
    associate (xcnode_onP => this%xcnode(:this%ncell_onP+1), &
                cnode_onP => this%cnode(:this%xcnode(this%ncell_onP+1)-1))
      call get_global_ragged_array (xcnode_onP, this%node_ip%global_index(cnode_onP), xcnode, cnode)
    end associate
  end subroutine get_global_cnode_array

  !! Creates the global ragged CFACE array on the IO process, 0-sized on others.
  subroutine get_global_cface_array (this, xcface, cface)
    class(unstr_mesh), intent(in) :: this
    integer, allocatable, intent(out) :: xcface(:), cface(:)
    associate (xcface_onP => this%xcface(:this%ncell_onP+1), &
                cface_onP => this%cface(:this%xcface(this%ncell_onP+1)-1))
      call get_global_ragged_array (xcface_onP, this%face_ip%global_index(cface_onP), xcface, cface)
    end associate
  end subroutine get_global_cface_array

  !! Auxiliary subroutine creates a global ragged array on the IO process,
  !! 0-sized on others, given a distributed ragged array.
  subroutine get_global_ragged_array (xarray_l, array_l, xarray, array)
    use parallel_communication, only: nPE, is_IOP, global_sum, collate, distribute
    integer, intent(in) :: xarray_l(:), array_l(:)
    integer, allocatable, intent(out) :: xarray(:), array(:)
    integer :: offset
    ASSERT(size(xarray_l) >= 1)
    ASSERT(xarray_l(1) == 1)
    ASSERT(size(array_l) == xarray_l(size(xarray_l))-1)
    ASSERT(all(xarray_l(2:) - xarray_l(:size(xarray_l)-1) >= 0))
#ifdef NAG_COMPILER_WORKAROUND
    !! NAGFOR 6.0(1052)/GCC 4.8.3 produce bad code under optimization (-O2)
    !! with the one-line allocation in parallel, getting the wrong value for
    !! the global_sum expression, apparently it gets the value of its argument
    !! instead.  Looking at the intermediate C code suggests that tha problem
    !! may not be here but in global_sum and perhaps a race condition?
    offset = global_sum(size(xarray_l)-1)
    allocate(xarray(1+merge(offset,0,is_IOP)))
#else
    allocate(xarray(1+merge(global_sum(size(xarray_l)-1),0,is_IOP)))
#endif
    offset = excl_prefix_sum(size(array_l))
    xarray(1) = 1
    call collate (xarray(2:), xarray_l(2:)+offset)
#ifdef NAG_COMPILER_WORKAROUND
    !! Same comments as above.
    offset = global_sum(size(array_l))
    allocate(array(merge(offset,0,is_IOP)))
#else
    allocate(array(merge(global_sum(size(array_l)),0,is_IOP)))
#endif
    call collate (array, array_l)
    if (is_IOP) then
      ASSERT(size(xarray) >= 1)
      ASSERT(xarray(1) == 1)
      ASSERT(all(xarray(2:) - xarray(:size(xarray)-1) >= 0))
      ASSERT(size(array) == xarray(size(xarray))-1)
    end if
  contains
    integer function excl_prefix_sum (n) result (psum)
      integer, intent(in) :: n
      integer :: j
      integer, allocatable :: array(:)
      allocate(array(merge(nPE,0,is_IOP)))
      call collate (array, n)
      if (is_IOP) then
        do j = 2, nPE
          array(j) = array(j) + array(j-1)
        end do
      end if
      call distribute (psum, array)
      psum = psum - n
    end function
  end subroutine get_global_ragged_array

 !! Writes to the tty and output file a profile of the distributed mesh:
 !! numbers of nodes, faces, and cells assigned to each processor; numbers
 !! of on-process and off-process objects.

  subroutine write_profile (this)

    use parallel_communication, only: nPE, broadcast, collate
    use truchas_logging_services

    class(unstr_mesh), intent(in) :: this

    integer :: n
    character(80) :: line
    integer, dimension(nPE) :: nnode_vec, nface_vec, ncell_vec
    integer, dimension(2,nPE) :: nvec, fvec, cvec

    call collate (nnode_vec, this%nnode)
    call collate (nface_vec, this%nface)
    call collate (ncell_vec, this%ncell)

    call broadcast (nnode_vec)
    call broadcast (nface_vec)
    call broadcast (ncell_vec)

    call TLS_info ('  UNSTR_MESH Profile:')
    write(line,fmt='(4x,a3,a,4a9)') 'PE', '|', 'nnode', 'nface', 'ncell'
    call TLS_info (line)
    call TLS_info ('    ---+'//repeat('-',27))
    do n = 1, nPE
      write(line,fmt='(4x,i3,a,3i9)') n, '|', nnode_vec(n),  nface_vec(n), ncell_vec(n)
      call TLS_info (line)
    end do

    call collate (nvec(1,:), this%node_ip%offP_size())
    call collate (nvec(2,:), this%node_ip%onP_size())
    call broadcast (nvec)

    call collate (fvec(1,:), this%face_ip%offP_size())
    call collate (fvec(2,:), this%face_ip%onP_size())
    call broadcast (fvec)

    call collate (cvec(1,:), this%cell_ip%offP_size())
    call collate (cvec(2,:), this%cell_ip%onP_size())
    call broadcast (cvec)

    call TLS_info ('  Mesh Communication Profile:')
    write(line,fmt='(4x,3x,1x,a11,2a16)')  'Nodes', 'Faces', 'Cells'
    call TLS_info (line)
    write(line,fmt='(4x,a3,a,3a16)') 'PE', '|', ('off-PE   on-PE', n=1,3)
    call TLS_info (line)
    call TLS_info ('    ---+'//repeat('-',48))
    do n = 1, nPE
      write(line,fmt='(4x,i3,a,3(i7,i9))') n, '|', nvec(:,n), fvec(:,n), cvec(:,n)
      call TLS_info (line)
    end do

  end subroutine write_profile

end module unstr_mesh_type
